import statistics
import collections
import os
import numpy as np
import tensorflow as tf
import time

from utensor_cgen.api.export import tflm_keras_export

raw_test_val = [[4.604608695652174, 2.716445652173913, 9.86191304347826, 2.117987804878049, 1.1571341463414635, 1.476768292682927, 3.071854119577681, 1.007738661118869, 2.046022480935033, 14.786542454511999, 54.70467364055721, 24.084149110612056, 21.663043478260867, 9.282608695652174, 11.771739130434783, 68.78048780487805, 451.70731707317077, 86.89024390243904, -7.076086956521739, 1.065217391304348, 0.9239130434782609, -71.58536585365854, -212.07317073170734, -113.47560975609757, 1.0],
    [10.038576086956521, 2.243065217391304, 2.8115108695652173, -3.492378048780488, 0.07591463414634174, -0.668719512195122, 2.2182529748676303, 1.6413887215680136, 3.914385150523728, 41.99496616060681, 114.20262735463373, 26.08170459291278, 17.26086956521739, 8.184782608695652, 11.17391304347826, 160.6707317073171, 334.51219512195127, 69.75609756097562, 2.3043478260869565, -2.9239130434782608, -4.913043478260869, -163.96341463414635, -487.74390243902445, -89.81707317073172, 1.0],
    [9.413217391304348, 5.67525, 0.1534782608695652, -1.4156097560975611, -7.621585365853659, -2.8557317073170734, 1.452780383628157, 1.6715107638180637, 2.9892869627103584, 30.861920015871004, 83.34010334655885, 24.920372394519287, 14.0, 11.391304347826086, 9.032608695652174, 104.93902439024392, 339.1463414634147, 69.81707317073172, 3.467391304347826, 1.4565217391304348, -4.83695652173913, -125.97560975609757, -369.26829268292687, -105.67073170731709, 1.0],
    [8.300673913043479, 3.94070652173913, 5.865739130434783, -0.06085365853658537, 10.705365853658538, -0.5234146341463415, 3.0035868605436304, 2.097626310629582, 3.8566941286403855, 24.83186445053912, 79.49248054132242, 22.837474184892834, 24.043478260869566, 10.815217391304348, 11.478260869565217, 119.08536585365854, 524.0243902439025, 95.85365853658537, -1.0108695652173914, -0.6304347826086957, -4.619565217391305, -90.54878048780489, -189.39024390243904, -83.78048780487805, 1.0],
    [3.789782608695652, 0.8452826086956522, 10.646858695652174, -1.5215243902439026, 0.2859146341463415, -0.5118292682926829, 0.7147918061869319, 1.4728381916635496, 0.47489959661197007, 32.55175621594281, 15.303112903233245, 12.503071666083757, 11.532608695652174, 10.304347826086957, 14.597826086956522, 524.7560975609756, 83.90243902439025, 151.03658536585368, -1.608695652173913, -7.978260869565217, 1.7391304347826086, -246.28048780487808, -190.12195121951223, -69.69512195121952, 1.0],
    [2.4924673913043476, 1.5259673913043479, 10.683097826086957, -0.15621951219512198, -6.950000000000001, -1.1244512195121952, 2.479828750498529, 0.5157781451649657, 0.9065028569901165, 4.1793245234950875, 23.410164820000492, 9.096084267806559, 10.695652173913043, 2.369565217391304, 11.804347826086957, 22.926829268292686, 16.463414634146343, 30.54878048780488, 0.05434782608695652, -2.032608695652174, 6.815217391304348, -18.719512195121954, -204.69512195121953, -42.987804878048784, 1.0],
    [7.702989130434783, -0.8136739130434782, 6.054423913043478, 1.6759756097560976, -1.180670731707317, 2.323475609756098, 2.8363381110023447, 2.497013706285479, 4.813311854994268, 48.852680784175256, 79.64828851122938, 30.76389758139261, 16.902173913043477, 8.728260869565217, 20.793478260869566, 165.85365853658539, 415.06097560975616, 154.57317073170734, -7.804347826086956, -9.48913043478261, -8.597826086956522, -164.14634146341464, -293.7804878048781, -128.109756097561, 1.0],
    [3.83525, -1.3644565217391305, 8.115586956521739, 0.4261585365853658, 4.255243902439025, -0.4998780487804879, 4.302100267231492, 2.3520610347615487, 3.4584011167740196, 38.3286909763806, 109.94531780948869, 24.60887771645217, 19.641304347826086, 3.891304347826087, 17.065217391304348, 120.3658536585366, 309.75609756097566, 74.75609756097562, -6.315217391304348, -7.83695652173913, -3.2065217391304346, -157.7439024390244, -425.24390243902445, -93.10975609756099, 1.0],
    [1.2774673913043477, 1.0451521739130434, 10.504358695652174, 2.0751219512195123, -1.1538414634146343, -4.941341463414634, 2.7895693670970925, 0.608847015714849, 1.0640044302613154, 7.747544071043902, 40.86267325005233, 23.862211650814082, 14.119565217391305, 3.5652173913043477, 11.347826086956522, 51.03658536585366, 185.1219512195122, 59.146341463414636, -1.4456521739130435, -2.293478260869565, 4.260869565217392, -19.085365853658537, -213.90243902439028, -113.71951219512196, 1.0],
    [6.994923913043478, -1.288945652173913, 4.345945652173913, -2.9913414634146345, -9.543597560975611, 1.5534756097560978, 3.8262140718777817, 2.0431467019017027, 5.401283132507201, 36.7979320780455, 125.58064692239657, 37.10996246578995, 18.51086956521739, 3.8478260869565215, 16.51086956521739, 131.64634146341464, 409.20731707317077, 162.07317073170734, -7.010869565217391, -6.141304347826087, -6.869565217391304, -151.15853658536588, -611.7073170731708, -151.28048780487805, 1.0],
    [5.00220652173913, -1.6197608695652173, 7.224695652173913, 0.12097560975609756, 8.324390243902439, 0.9748170731707319, 4.269874939191947, 2.2164353705061304, 4.115021337096996, 40.2455110805882, 108.83126408243739, 25.118494028272195, 16.32608695652174, 5.173913043478261, 18.380434782608695, 145.7926829268293, 437.07317073170736, 107.25609756097562, -7.33695652173913, -7.554347826086956, -6.010869565217392, -174.75609756097563, -546.6463414634147, -111.89024390243904, 1.0],
    [1.8489239130434783, 0.7077934782608696, 10.456586956521738, 2.3781707317073173, -0.17262195121951213, -4.7630487804878054, 2.8505332587450654, 0.6877759874025844, 1.1118368226300444, 8.873569957927312, 39.199376732225474, 33.048675588165985, 11.16304347826087, 2.641304347826087, 12.282608695652174, 46.646341463414636, 158.41463414634148, 50.0609756097561, -1.8152173913043477, -1.8043478260869565, 6.010869565217392, -31.09756097560976, -262.8048780487805, -204.69512195121953, 1.0],]
raw_test_res = [0, 1, 1, 1, 0, 2, 2, 2, 0, 2, 2, 0]

def compute_features(file_path, overlap = 1):
    assert(overlap > 0.0 and overlap <= 1.0)
    print("Computing features from data...")

    features_val = []
    with open(file_path, "r") as file:
        lines = file.readlines()[1:]

        idx = 0
        while idx < len(lines):
            current_rows = lines[idx: idx+1000]
            

            current_rows = [x.rstrip("\n").split(',')[1:] for x in current_rows]
            current_rows = [[float(x) if '.' in x else int(x) for x in xs] for xs in current_rows]
            current_cols = list(map(list,zip(*current_rows)))

            avgs = tuple([statistics.mean(x) for x in current_cols[:6]])
            stds = tuple([statistics.stdev(x) for x in current_cols[:6]])
            maxes = tuple([max(x) for x in current_cols[:6]])
            mins = tuple([min(x) for x in current_cols[:6]])
            hand = 0 if statistics.mean(current_cols[7]) < 0.5 else 1

            try:
                state = statistics.mode(current_cols[6])
            except statistics.StatisticsError:
                state = collections.Counter(current_cols[6]).most_common(1)[0][0]
            
            features_val.append([
                avgs[0],avgs[1],avgs[2],avgs[3],avgs[4],avgs[5],
                stds[0],stds[1],stds[2],stds[3],stds[4],stds[5],
                maxes[0],maxes[1],maxes[2],maxes[3],maxes[4],maxes[5],
                mins[0],mins[1],mins[2],mins[3],mins[4],mins[5],
                hand,
                state
            ])
            
            idx += int(1000*overlap)
    print("Computing features from data end!")
    return features_val


#input_file_path = "../../../Raccolta Dati/export/lorenzotracce_corte_left_1.csv"
input_file_path = "../../../Raccolta Dati/export/sum.csv"

start_time = time.time()

# check if computed features are stored in a file
if os.path.isfile("./comp_feats.txt"):
    with open("./comp_feats.txt", "r") as f:
        feats = [[int(e) if i == 24 or i == 25 else float(e) for i, e in enumerate(l[:-1].split(", "))] for l in f.readlines()]
        data = np.array(feats)
else:
    feats = compute_features(input_file_path,1.0)
    with open("./comp_feats.txt", "w+") as f:
        [f.write(", ".join(str(e) for e in l)+"\n") for l in feats]
    data = np.array(feats)

np.random.shuffle(data)
split_idx = int(data.shape[0] * 0.8)
training, test = data[:split_idx, :], data[split_idx:, :]
x_train, y_train = training[:, :-1], training[:, -1]
x_test, y_test = test[:, :-1], test[:, -1]

x_train = x_train.astype(np.float32)
x_test = x_test.astype(np.float32)

print("")
print("training shape:", x_train.shape)
print("test shape:", x_test.shape)
print("")

model = tf.keras.Sequential([
    tf.keras.layers.Dense(256, input_dim=25, activation="relu"),
    tf.keras.layers.Dense(256, activation="relu"),
    tf.keras.layers.Dense(3, "softmax")
])
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=["sparse_categorical_accuracy"])
model.summary()

model.fit(x_train, y_train, epochs=100, batch_size=32)
loss, accuracy = model.evaluate(x_test, y_test)
print("Model trained with:")
print(f"loss {loss*100}%, accuracy {accuracy*100}%")

# Test some samples
preds = model.predict(raw_test_val)
print("Test some predefined data:")
print(f"predictions:     {preds}")
print(f"actual labels:   {list([tf.keras.backend.eval(tf.argmax(p)) for p in preds])}")
print(f"expected labels: {raw_test_res}")

print()
print(f"Complete in {time.time()-start_time}s")

print("Saving model...")
model.save("mlp_handwash_model")
print("Model saved!")

print("Converting model to tflite...")
converter = tf.lite.TFLiteConverter.from_saved_model("mlp_handwash_model")
tflite_model = converter.convert()
with open("mlp_handwash_model.tflite", "wb") as f:
    f.write(tflite_model)
print("Edn Converting model to tflite...")

if False:
    def rep_dataset_gen():
        for _ in range(128):
            idx = np.random.randint(0, x_test.shape[0]-1)
            sample = x_test[idx]
            sample = sample[tf.newaxis, ...]
            sample = tf.cast(sample, dtype=tf.float32)
            yield [sample]

    tflm_keras_export("mlp_handwash_model", model_name="mlp_hw_model", representive_dataset=rep_dataset_gen)
